# REST-ASMR: Response to Environmental & Sensory Triggers

**A Multimodal Dataset and Deep Learning Pipeline for ASMR and Nature-Induced Relaxation**

[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Data License: CC BY 4.0](https://img.shields.io/badge/Data_License-CC_BY_4.0-blue.svg)](https://creativecommons.org/licenses/by/4.0/)

This repository contains the official data processing, feature extraction, and machine learning pipeline for the **REST-ASMR** dataset. REST-ASMR is a synchronized multimodal resource containing photoplethysmography (PPG), continuous behavioral annotations, and audiovisual stimuli designed to model and encourage research into Autonomous Sensory Meridian Response (ASMR) contrasted against Nature relaxation controls.

## Repository Structure

- `main.py` - The main orchestrator. It executes the backend pipeline: log parsing, audiovisual/PPG feature extraction, and 4-fold cross-validation training for the Deep Learning (BiLSTM, multimodal and unimodal ablations) and XGBoost models.
- `REST_ASMR_Analysis.ipynb` - The frontend Jupyter Notebook dashboard. It generates the statistical validations, Tinglegrams, and performance tables reported in the manuscript. The user can use it to visualize the recent run results as well.
- `pipeline/` - Contains the Python scripts utilized by the orchestrator.
- `data/` - The directory for raw dataset inputs and serialized inference outputs.

## Dataset Setup

The dataset files must be downloaded from our permanent academic repository.

1. Download the raw REST-ASMR dataset from .
2. Extract the contents and place them into the following empty folders within this repository:
   - `./data/log/` (Contains the 34 behavioral event logs)
   - `./data/ppg/` (Contains the 34 raw PPG recordings)
   - `./data/stim/` (Contains the 8 `.avi` video stimuli)

## Running the Pipeline

**1. Install Dependencies**

```bash
pip install -r requirements.txt
```

**2. Execute the Backend Orchestrator**
Run the main pipeline script. This will parse the raw logs, extract 544-dimensional multimodal features (ResNet-18, MFCCs, normalized PPG) at 10 Hz, and train the Deep Learning models. The 4 fold results will be printed.

```bash
python main.py
```

**3. Evaluate the Dashboard**
Launch and run the interactive notebook cells to view the statistical analysis and visualizations:

```bash
jupyter notebook REST_ASMR_Analysis.ipynb
```

## Reproducibility Option

Deep learning models trained on GPU hardware utilizing CuDNN backends exhibit inherent non-determinism due to parallelized atomic operations during backpropagation. To enable strict reproducibility of the manuscript's findings, the exact serialized inference artifact generated by the authors have been provided in the repository (`paper_fusion_results_4_fold.pkl`).

In the notebook:

- Set `USE_PAPER_ARTIFACTS = True` to load the provided manuscript artifact and reproduce the tables, $p$-values, and video-level accuracy reported in the paper.
- Set `USE_PAPER_ARTIFACTS = False` to evaluate the new model weights generated if you chose to re-run `main.py`.

## Citation

If you utilize the REST-ASMR dataset or this pipeline in your research, please cite our work:

```bibtex
@article{das2026restasmr,
  title={REST-ASMR: A Multimodal dataset of physiological and continuous behavioral responses to ASMR and nature stimuli},
  author={Das, Tushar and Hozaki, Daigo and Singh, Koushlendra Kumar and Kondo, Hirohito M.},
  journal={Scientific Data},
  year={2026},
  note={Under Review}
}
```

## License

- The **Code** in this repository is licensed under the [MIT License](LICENSE).
- The **REST-ASMR Dataset** (Log files, PPG, Stimuli) is licensed under a [Creative Commons Attribution 4.0 International License (CC BY 4.0)](https://creativecommons.org/licenses/by/4.0/).
