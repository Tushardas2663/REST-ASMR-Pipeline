{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "567c84be",
   "metadata": {},
   "source": [
    "# REST-ASMR: Statistical Analysis & Technical Validation\n",
    "\n",
    ">  **PREREQUISITE:** This notebook provides the visual and statistical validation for the REST-ASMR dataset. Before running any cells below, ensure to execute the backend orchestrator by running `!python main.py`\n",
    "> \n",
    "> The orchestrator processes the raw signals, extracts the deep learning features, and populates the `./data/features/` and `./data/` directories with the necessary artifacts that this notebook analyzes.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "951d1364",
   "metadata": {},
   "source": [
    "### Behavioral Inter-Subject Agreement (ISA)\n",
    "\n",
    "**Methodology:**\n",
    "To verify that the ASMR tingling sensations were stimulus-locked with a cross-participant consensus, a Leave-One-Out Inter-Subject Agreement (LOO-ISA) was calculated using Fisher's Z-transformed Pearson's $r$ and bootstrap resaampling to ascertain if the group agreement was significantly greater than zero.\n",
    "\n",
    "This code outputs the 95% Confidence Intervals (CIs) and $p$-values reported in the manuscript."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baa27b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "import scipy.stats\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "LABEL_FOLDER = \"./data/features\"\n",
    "\n",
    "def calculate_bootstrapped_isa():\n",
    "    \n",
    "    print(\" BOOTSTRAPPED BEHAVIORAL INTER-SUBJECT AGREEMENT (ISA) \")\n",
    "   \n",
    "    \n",
    "    label_files = glob.glob(os.path.join(LABEL_FOLDER, \"y_*.npy\"))\n",
    "    if not label_files:\n",
    "        print(\"No label files found.\")\n",
    "        return\n",
    "\n",
    "    all_vids = [\"vid1\", \"vid2\", \"vid3\", \"vid4\", \"vid5\", \"vid6\", \"vid7\", \"vid8\"]\n",
    "    video_labels = {vid: [] for vid in all_vids}\n",
    "    \n",
    "    for f in label_files:\n",
    "        filename = os.path.basename(f)\n",
    "        vid = filename.replace(\".npy\", \"\").split(\"_\")[2]\n",
    "        video_labels[vid].append(np.load(f))\n",
    "        \n",
    "    for condition, vids in [(\"ASMR (Tingle)\", [\"vid1\", \"vid2\", \"vid3\", \"vid4\"]), \n",
    "                            (\"Nature (Pleasantness)\", [\"vid5\", \"vid6\", \"vid7\", \"vid8\"])]:\n",
    "        print(f\"\\n  {condition}   \")\n",
    "        \n",
    "        for vid in vids:\n",
    "            arrays = video_labels[vid] \n",
    "            if len(arrays) < 2:\n",
    "                print(\"Wrong\")\n",
    "                continue\n",
    "            \n",
    "                \n",
    "            \n",
    "            min_len = min(len(arr) for arr in arrays)\n",
    "            stacked_arrays = np.vstack([arr[:min_len] for arr in arrays])\n",
    "            \n",
    "            num_subjects = stacked_arrays.shape[0]\n",
    "            subject_z_scores = []\n",
    "            \n",
    "           \n",
    "            for i in range(num_subjects):\n",
    "                target_sub = stacked_arrays[i]\n",
    "                other_subs_idx = [j for j in range(num_subjects) if j != i]\n",
    "                mean_others = np.mean(stacked_arrays[other_subs_idx], axis=0)\n",
    "                \n",
    "                if np.std(target_sub) > 0 and np.std(mean_others) > 0:\n",
    "                    r, _ = scipy.stats.pearsonr(target_sub, mean_others)\n",
    "                    if not np.isnan(r):\n",
    "                        subject_z_scores.append(np.arctanh(r))\n",
    "                        \n",
    "            if not subject_z_scores:\n",
    "                print(f\"  > {vid.upper()}: Insufficient variance for ISA.\")\n",
    "                continue\n",
    "\n",
    "            \n",
    "            np.random.seed(42) \n",
    "            n_iterations = 10000\n",
    "            z_array = np.array(subject_z_scores)\n",
    "            \n",
    "            \n",
    "            bootstrap_means = [np.mean(np.random.choice(z_array, size=len(z_array), replace=True)) \n",
    "                               for _ in range(n_iterations)]\n",
    "            \n",
    "            \n",
    "            mean_z = np.mean(bootstrap_means)\n",
    "            mean_r = np.tanh(mean_z)\n",
    "            \n",
    "            \n",
    "            ci_lower_z, ci_upper_z = np.percentile(bootstrap_means, [2.5, 97.5])\n",
    "            ci_lower_r = np.tanh(ci_lower_z)\n",
    "            ci_upper_r = np.tanh(ci_upper_z)\n",
    "            \n",
    "         \n",
    "            p_val = np.sum(np.array(bootstrap_means) <= 0) / n_iterations\n",
    "            \n",
    "            \n",
    "            sig_star = \"***\" if p_val < 0.001 else \"**\" if p_val < 0.01 else \"*\" if p_val < 0.05 else \"ns\"\n",
    "            \n",
    "            print(f\"  > {vid.upper()} Mean ISA (r) : {mean_r:.4f} {sig_star}\")\n",
    "            print(f\"      95% CI       : [{ci_lower_r:.4f}, {ci_upper_r:.4f}]\")\n",
    "            print(f\"      p-value      : {p_val:.4f} (Bootstrapped)\")\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    calculate_bootstrapped_isa()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cc5495a",
   "metadata": {},
   "source": [
    "### Physiological Signal Integrity & Cardiovascular Response\n",
    "\n",
    "\n",
    "**Methodology:**\n",
    "This section validates the biological efficacy of the stimuli by assessing the subject-wise parasympathetic cardiovascular deceleration associated with the ASMR state. It utilizes a 5-s sliding window approach with Spearman rank correlation and paired-samples t-test to stastically quantify the difference in ASMR vs. Nature induced relaxation.\n",
    "\n",
    "This code outputs the statistical significance ($p$-value) and generates the manuscript **Figure**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99f898a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import scipy.stats\n",
    "import scipy.signal\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "\n",
    "RAW_PPG_FOLDER = \"./data/ppg\"\n",
    "LABEL_FOLDER = \"./data/features\" \n",
    "LOG_FOLDER = \"./data/log\"\n",
    "WINDOW_SEC = 5.0   \n",
    "STEP_SEC = 1.0     \n",
    "\n",
    "def load_raw_ppg(filepath):\n",
    "   \n",
    "    encodings = ['shift_jis', 'utf-8', 'cp1252']\n",
    "    for enc in encodings:\n",
    "        try:\n",
    "            df = pd.read_csv(filepath, sep='\\t', skiprows=8, encoding=enc, on_bad_lines='skip')\n",
    "            if df.shape[1] >= 3:\n",
    "                df = df.iloc[:, [0, 2]] \n",
    "                df.columns = ['Time_Min', 'Val']\n",
    "                df = df[pd.to_numeric(df['Time_Min'], errors='coerce').notnull()]\n",
    "                df['Time_Sec'] = df['Time_Min'].astype(float) * 60.0\n",
    "                return df\n",
    "            else:\n",
    "                print(\"Wrong shape\")\n",
    "        except:\n",
    "            print(\"Wrong!\")\n",
    "            continue\n",
    "    return None\n",
    "\n",
    "def get_window_features(chunk, fs=2000.0):\n",
    "   \n",
    "  \n",
    "    sos = scipy.signal.butter(2, [0.5, 4.0], btype='bandpass', fs=fs, output='sos')\n",
    "    filtered = scipy.signal.sosfiltfilt(sos, chunk)\n",
    "    \n",
    "   \n",
    "    peaks, _ = scipy.signal.find_peaks(filtered, distance=int(fs*0.35), prominence=0.5)\n",
    "  \n",
    "    \n",
    "    if len(peaks) < 4: return np.nan, np.nan \n",
    "    \n",
    "    \n",
    "    rr_intervals = np.diff(peaks) / fs * 1000.0   \n",
    "    \n",
    "    \n",
    "    valid_rr = rr_intervals[(rr_intervals > 300) & (rr_intervals < 1300)]\n",
    "    \n",
    "    if len(valid_rr) < 3: return np.nan, np.nan\n",
    "    \n",
    "    hr = 60000.0 / np.mean(valid_rr)  \n",
    "    rmssd = np.sqrt(np.mean(np.diff(valid_rr)**2))\n",
    "    \n",
    "    return hr, rmssd\n",
    "\n",
    "def compare_asmr_nature_physiology():\n",
    "    print(\" STARTING SLIDING WINDOW COMPARISON (ASMR vs NATURE) \")\n",
    "    \n",
    "   \n",
    "    timeline_map = {} \n",
    "    log_files = glob.glob(os.path.join(LOG_FOLDER, \"*.log\"))\n",
    "    for log_f in log_files:\n",
    "        try:\n",
    "            sub_id = int(os.path.basename(log_f).split('-')[0])\n",
    "            with open(log_f, 'r', encoding='utf-8', errors='ignore') as f: lines = f.readlines()\n",
    "            header_idx = next(i for i, l in enumerate(lines) if l.startswith(\"Subject\"))\n",
    "            ldf = pd.read_csv(log_f, sep='\\t', skiprows=header_idx)\n",
    "            vids = ldf[ldf['Event Type'] == 'Video'].reset_index(drop=True)\n",
    "            for i, row in vids.iterrows():\n",
    "                dur = 55.7 if (i==0) else 60.0\n",
    "                timeline_map[(sub_id, row['Code'])] = (row['Time']/10000.0, dur)\n",
    "        except: \n",
    "            print(\"Wrong!!\")\n",
    "            continue\n",
    "\n",
    "    results = []\n",
    "    subjects = sorted(list(set([k[0] for k in timeline_map.keys()])))\n",
    "    \n",
    "    for sub in tqdm(subjects, desc=\"Analyzing Subjects\"):\n",
    "        raw_path = os.path.join(RAW_PPG_FOLDER, f\"{str(sub).zfill(3)}_csv.txt\")\n",
    "        if not os.path.exists(raw_path): \n",
    "            print(\"WRONG\")\n",
    "            continue\n",
    "        \n",
    "        df_ppg = load_raw_ppg(raw_path)\n",
    "        if df_ppg is None: \n",
    "            print(\"WRONG\")\n",
    "            continue\n",
    "        \n",
    "        \n",
    "        data_buckets = {\n",
    "            'ASMR': {'hr': [], 'hrv': [], 'rating': []},\n",
    "            'Nature': {'hr': [], 'hrv': [], 'rating': []}\n",
    "        }\n",
    "        \n",
    "        all_videos = [\"vid1\", \"vid2\", \"vid3\", \"vid4\", \"vid5\", \"vid6\", \"vid7\", \"vid8\"]\n",
    "        \n",
    "        for vid in all_videos:\n",
    "            if (sub, vid) not in timeline_map: continue\n",
    "            \n",
    "           \n",
    "            category = 'Nature' if vid in [\"vid5\", \"vid6\", \"vid7\", \"vid8\"] else 'ASMR'\n",
    "            \n",
    "            start_sec, dur_sec = timeline_map[(sub, vid)]\n",
    "            \n",
    "           \n",
    "            y_path = os.path.join(LABEL_FOLDER, f\"y_{sub}_{vid}.npy\")\n",
    "            if not os.path.exists(y_path): continue\n",
    "            y_data = np.load(y_path)\n",
    "            \n",
    "            \n",
    "            curr_time = start_sec + 5.0\n",
    "            end_time = start_sec + dur_sec\n",
    "            \n",
    "            while curr_time + WINDOW_SEC < end_time:\n",
    "                \n",
    "                mask = (df_ppg['Time_Sec'] >= curr_time) & (df_ppg['Time_Sec'] < curr_time + WINDOW_SEC)\n",
    "                chunk = df_ppg.loc[mask, 'Val'].values\n",
    "                \n",
    "                \n",
    "              \n",
    "                rel_start = curr_time - (start_sec + 5.0)\n",
    "                idx_start = int(rel_start * 10)    #as labels are in 10 hz and rel_start in s\n",
    "                idx_end = int((rel_start + WINDOW_SEC) * 10)\n",
    "                \n",
    "                if idx_end <= len(y_data) and len(chunk) > 1000:\n",
    "                    hr, hrv = get_window_features(chunk)\n",
    "                    avg_rating = np.mean(y_data[idx_start:idx_end])\n",
    "                    \n",
    "                    if not np.isnan(hr) and not np.isnan(hrv):\n",
    "                        data_buckets[category]['hr'].append(hr)\n",
    "                        data_buckets[category]['hrv'].append(hrv)\n",
    "                        data_buckets[category]['rating'].append(avg_rating)\n",
    "                \n",
    "                curr_time += STEP_SEC\n",
    "\n",
    "       \n",
    "        for cat in ['ASMR', 'Nature']:\n",
    "            hrs = data_buckets[cat]['hr']\n",
    "            hrvs = data_buckets[cat]['hrv']\n",
    "            rats = data_buckets[cat]['rating']\n",
    "            \n",
    "            if len(hrs) > 10 and np.std(rats) > 0:\n",
    "                r_hr, p_hr = scipy.stats.spearmanr(hrs, rats)\n",
    "                r_hrv, p_hrv = scipy.stats.spearmanr(hrvs, rats)\n",
    "                \n",
    "                results.append({\n",
    "                    'Subject': sub,\n",
    "                    'Condition': cat,\n",
    "                    'r_HR': r_hr,\n",
    "                    'r_HRV': r_hrv,\n",
    "                    'p_HR': p_hr\n",
    "                })\n",
    "\n",
    "    \n",
    "    df_res = pd.DataFrame(results)\n",
    "    \n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "    \n",
    "   \n",
    "    sns.boxplot(x='Condition', y='r_HR', data=df_res, ax=ax, palette=\"Set2\")\n",
    "    sns.stripplot(x='Condition', y='r_HR', data=df_res, color='black', alpha=0.5, ax=ax)\n",
    "    \n",
    "    \n",
    "    ax.set_title(\"Correlation: HR vs. Rating (ASMR vs Nature)\", fontweight='bold')\n",
    "    ax.set_ylabel(\"Spearman Correlation (r)\")\n",
    "    ax.axhline(0, color='red', linestyle='--')\n",
    "    \n",
    "   \n",
    "    plt.tight_layout()\n",
    "    #plt.savefig(\"asmr_hr_stat_test.svg\")\n",
    "    plt.show()\n",
    "    \n",
    "    asmr_hr = df_res[df_res['Condition']=='ASMR'].set_index('Subject')['r_HR']\n",
    "    nat_hr = df_res[df_res['Condition']=='Nature'].set_index('Subject')['r_HR']\n",
    "    \n",
    "   \n",
    "    common = pd.concat([asmr_hr, nat_hr], axis=1, join='inner')\n",
    "    common.columns = ['ASMR', 'Nature']\n",
    "    \n",
    "    t_stat, p_val = scipy.stats.ttest_rel(common['ASMR'], common['Nature'])\n",
    "    \n",
    "    print(\"\\n STATISTICAL COMPARISON (Paired T-Test) \")\n",
    "    print(f\"Mean HR Correlation (ASMR):   {common['ASMR'].mean():.4f}\")\n",
    "    print(f\"Mean HR Correlation (Nature): {common['Nature'].mean():.4f}\")\n",
    "    print(f\"Difference P-Value: {p_val:.4f}, t-stat: {t_stat} \")\n",
    "    asmr_z = np.arctanh(common['ASMR'])\n",
    "    nat_z = np.arctanh(common['Nature'])\n",
    "    \n",
    "   \n",
    "    t_stat2, p_val2 = scipy.stats.ttest_rel(asmr_z, nat_z)\n",
    "    print(f\"Corrected Difference P-Value: {p_val2:.4f}, t-stat: {t_stat2} \")\n",
    "    if p_val < 0.05:\n",
    "        print(\"SIGNIFICANT DIFFERENCE found between physiological response to ASMR vs Nature.\")\n",
    "    else:\n",
    "        print(\"No significant global difference in correlation magnitude.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    compare_asmr_nature_physiology()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f82ca8b5",
   "metadata": {},
   "source": [
    "### Group-Level Temporal Agreement\n",
    "\n",
    "**Visualization Methodology:**\n",
    "This visualizes the temporal consensus of the ASMR tingling sensation across all active participants for the two highest-performing stimuli (Video 2 and Video 3). The figure displays **Mean Signal** with a solid line and the **Standard Error of Mean** using the shaded region.\n",
    "\n",
    "\n",
    "This code outputs the manuscript Figure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90596513",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "LABEL_FOLDER = \"./data/features\"\n",
    "\n",
    "def get_mean_timeseries(vid_target):\n",
    "    label_files = glob.glob(os.path.join(LABEL_FOLDER, f\"y_*_{vid_target}.npy\"))\n",
    "    if not label_files:\n",
    "        return None, None, None\n",
    "        \n",
    "    subject_arrays = []\n",
    "    for f in label_files:\n",
    "        y_data = np.load(f)\n",
    "        if np.max(y_data) > 0: \n",
    "            subject_arrays.append(y_data)\n",
    "\n",
    "    if not subject_arrays: return None, None, None\n",
    "\n",
    "    min_len = min(len(arr) for arr in subject_arrays)\n",
    "    stacked_data = np.vstack([arr[:min_len] for arr in subject_arrays])\n",
    "    \n",
    "    mean_signal = np.mean(stacked_data, axis=0)\n",
    "    sem_signal = np.std(stacked_data, axis=0) / np.sqrt(stacked_data.shape[0])\n",
    "    time_axis = np.arange(min_len) / 10.0 \n",
    "    \n",
    "    return time_axis, mean_signal, sem_signal\n",
    "\n",
    "def plot_dual_asmr_timeseries():\n",
    "    \n",
    "    \n",
    "    t_vid2, mean_vid2, sem_vid2 = get_mean_timeseries(\"vid2\")\n",
    "    t_vid3, mean_vid3, sem_vid3 = get_mean_timeseries(\"vid3\")\n",
    "    \n",
    "    if t_vid2 is None or t_vid3 is None:\n",
    "        print(\"Data missing\")\n",
    "        return\n",
    "\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5), sharey=True)\n",
    "    sns.set_theme(style=\"whitegrid\")\n",
    "    \n",
    "    \n",
    "    axes[0].plot(t_vid2, mean_vid2, color='firebrick', linewidth=2.5, label='Mean Intensity')\n",
    "    axes[0].fill_between(t_vid2, np.maximum(0, mean_vid2 - sem_vid2), mean_vid2 + sem_vid2, color='lightcoral', alpha=0.4, label='Standard Error')\n",
    "    axes[0].set_title(\"A. Group-Level Temporal Agreement: Video 2\", fontweight='bold')\n",
    "    axes[0].set_ylabel(\"Mean Subjective Rating (0-3)\", fontweight='bold')\n",
    "    axes[0].set_xlabel(\"Trial Time (Seconds)\", fontweight='bold')\n",
    "    axes[0].set_ylim(0, 3.0)\n",
    "    axes[0].legend(loc=\"upper left\")\n",
    "\n",
    "    \n",
    "    axes[1].plot(t_vid3, mean_vid3, color='darkred', linewidth=2.5, label='Mean Intensity')\n",
    "    axes[1].fill_between(t_vid3, np.maximum(0, mean_vid3 - sem_vid3), mean_vid3 + sem_vid3, color='indianred', alpha=0.4, label='Standard Error')\n",
    "    axes[1].set_title(\"B. Group-Level Temporal Agreement: Video 3\", fontweight='bold')\n",
    "    axes[1].set_xlabel(\"Trial Time (Seconds)\", fontweight='bold')\n",
    "    axes[1].legend(loc=\"upper left\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    #plt.savefig(\"Figure1_DualASMR_TimeSeries.svg\", dpi=300)\n",
    "    plt.show()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    plot_dual_asmr_timeseries()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee0985d5",
   "metadata": {},
   "source": [
    "### 5. Multimodal Predictive Modeling: Reproducibility Setup\n",
    "\n",
    "**Methodology & Hardware Variance:**\n",
    "Deep learning models trained on GPU hardware utilizing CuDNN backends exhibit  non-determinism due to parallelized atomic operations during backpropagation. To account for this fact and enable strict reproducibility of the manuscript's findings, this evaluation pipeline relies on serialized inference artifacts (`.pkl` files).\n",
    "\n",
    "**Execution Modes:**\n",
    "This data-loading block controls the artifact pipeline for the remainder of the notebook:\n",
    "* `USE_PAPER_ARTIFACTS = True`: Loads the exact file (`paper_fusion_results_4_fold.pkl`) generated by the authors and utilized in the manuscript to reproduce all tables and $p$-values.\n",
    "* `USE_PAPER_ARTIFACTS = False`: Loads dynamic artifacts (`fusion_results.pkl`) generated by a fresh run of the orchestrator to evaluate newly trained network weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b66ee115",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "\n",
    "\n",
    "USE_PAPER_ARTIFACTS = True\n",
    "\n",
    "if USE_PAPER_ARTIFACTS:\n",
    "    file_path = './data/paper_fusion_results_4_fold.pkl'  #saved by the authors\n",
    "    print(\"MODE: Reproducing Manuscript Results\")\n",
    "else:\n",
    "    file_path = './data/fusion_results.pkl'            # generated now by a fresh run of the main orchestrator\n",
    "    print(\"MODE: Evaluating Newly Trained Model\")\n",
    "\n",
    "try:\n",
    "    with open(file_path, 'rb') as f:\n",
    "        results = pickle.load(f)\n",
    "    print(f\"Successfully loaded {len(results)} trial sequences from '{file_path}'\")\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: '{file_path}' not found.\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03930324",
   "metadata": {},
   "source": [
    "### 5. Manuscript Performance Reconstruction \n",
    "\n",
    "**Methodology:**\n",
    " \n",
    "To replicate the manuscript results, ensure that the above cell with \"USE_PAPER_ARTIFACTS = True\" was run.\n",
    "This cell allows strict replication and accounts for the fact that Deep learning models trained on GPU hardware utilizing CuDNN backends exhibit inherent non-determinism due to parallelized atomic operations during backpropagation.\n",
    "\n",
    "This section reconstructs the exact fold-by-fold performance metrics and the global aggregated summary (Means $\\pm$ Standard Deviations) from the saved model inference artifacts reported. These metrics form the multimodal model results reported in the manuscript."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f73cd842",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reconstruct_full_training_summary(test_results):\n",
    "    print(\"  RECONSTRUCTING 4-FOLD METRICS FROM RESULTS  \")\n",
    "    \n",
    "   \n",
    "    fold_vid_map = {\n",
    "        0: [\"vid1\", \"vid5\"],\n",
    "        1: [\"vid2\", \"vid6\"],\n",
    "        2: [\"vid3\", \"vid7\"],\n",
    "        3: [\"vid4\", \"vid8\"]\n",
    "    }\n",
    "    \n",
    "    metrics_history = {\n",
    "        'accuracy': [],\n",
    "        'macro_f1': [],\n",
    "        'class1_f1': [],\n",
    "        'class0_f1': [],\n",
    "        'nature_spec': [],\n",
    "        'precision': [],\n",
    "        'recall': []\n",
    "    }\n",
    "    \n",
    "    for fold in range(4):\n",
    "       \n",
    "        fold_vids = fold_vid_map[fold]\n",
    "        fold_trials = [tr for tr in test_results if tr['vid_name'] in fold_vids]\n",
    "        \n",
    "        if not fold_trials:\n",
    "            print(f\"Fold {fold+1} - No data found in results.\")\n",
    "            continue\n",
    "            \n",
    "        \n",
    "        final_truths = np.concatenate([tr['truths'] for tr in fold_trials])\n",
    "        final_preds = np.concatenate([tr['preds'] for tr in fold_trials])\n",
    "     \n",
    "        nature_trials = [tr for tr in fold_trials if tr['is_nature'] == 1]\n",
    "        if nature_trials:\n",
    "            nature_truths = np.concatenate([tr['truths'] for tr in nature_trials])\n",
    "            nature_preds = np.concatenate([tr['preds'] for tr in nature_trials])\n",
    "            fold_nat_spec = accuracy_score(nature_truths, nature_preds)\n",
    "        else:\n",
    "            fold_nat_spec = 1.0\n",
    "            \n",
    "        \n",
    "        report_dict = classification_report(final_truths, final_preds, output_dict=True, zero_division=0)\n",
    "        \n",
    "        print(f\"  > Fold {fold+1} Macro F1: {report_dict['macro avg']['f1-score']:.4f}\")\n",
    "        \n",
    "       \n",
    "        metrics_history['accuracy'].append(report_dict['accuracy'])\n",
    "        metrics_history['macro_f1'].append(report_dict['macro avg']['f1-score'])\n",
    "        metrics_history['class1_f1'].append(report_dict.get('1', {}).get('f1-score', 0))\n",
    "        metrics_history['class0_f1'].append(report_dict.get('0', {}).get('f1-score', 0))\n",
    "        metrics_history['precision'].append(report_dict['macro avg']['precision'])\n",
    "        metrics_history['recall'].append(report_dict['macro avg']['recall'])\n",
    "        metrics_history['nature_spec'].append(fold_nat_spec)\n",
    "\n",
    "    print()\n",
    "    print(\"      FINAL FUSION SUMMARY (4-FOLD)      \")\n",
    "    print()\n",
    "    \n",
    "    def p_stat(name, key):\n",
    "        mean_val = np.mean(metrics_history[key])\n",
    "        std_val = np.std(metrics_history[key])\n",
    "        print(f\"{name:<25} : {mean_val:.4f} (+/- {std_val:.4f})\")\n",
    "\n",
    "    p_stat(\"Global Accuracy\", 'accuracy')\n",
    "    p_stat(\"Global Macro F1\", 'macro_f1')\n",
    "    p_stat(\"Global Macro Precision\", 'precision')\n",
    "    p_stat(\"Global Macro Recall\", 'recall')\n",
    "    p_stat(\"Nature Specificity\", 'nature_spec')\n",
    "    print()\n",
    "\n",
    "\n",
    "\n",
    "reconstruct_full_training_summary(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e975047",
   "metadata": {},
   "source": [
    "### Multimodal ML Predictive Utility & Statistical Significance\n",
    "\n",
    "**Methodology:**\n",
    "This section performs two non-parametric statistical tests on the aggregated out-of-fold predictions of the multimodal BiLSTM model. 95% CIs are estimated via non-parametric subject-level bootstrap resampling (2,000 iterations). Significance versus chance is assessed using a sequence-level permutation test (5,000 iterations). Note: this snippet requires the results generated by runnnig the multimodal model via the main orchestrator.\n",
    "\n",
    "\n",
    "This code outputs the statistical metrics reported in manuscript Table. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6814fa24",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n",
    "from tqdm import tqdm \n",
    "\n",
    "def run_statistics(test_results, n_boot=2000, n_perm=5000):\n",
    "    print()\n",
    "    print(\"STATISTICAL ANALYSIS (BOOTSTRAP & PERMUTATION) \")\n",
    "    print()\n",
    "    \n",
    "    unique_subjs = list(set([tr['subj'] for tr in test_results]))\n",
    "    \n",
    "    \n",
    "    \n",
    " \n",
    "    def calc_all_metrics(truths_list, preds_list, is_nat_list):\n",
    "        flat_t = np.concatenate(truths_list)\n",
    "        flat_p = np.concatenate(preds_list)\n",
    "        \n",
    "        acc = accuracy_score(flat_t, flat_p)\n",
    "        prec, rec, f1, _ = precision_recall_fscore_support(flat_t, flat_p, average='macro', zero_division=0)\n",
    "        \n",
    " \n",
    "        nat_t, nat_p = [], []\n",
    "        for i in range(len(is_nat_list)):\n",
    "            if is_nat_list[i] == 1:\n",
    "                nat_t.extend(truths_list[i])\n",
    "                nat_p.extend(preds_list[i])\n",
    "                \n",
    "        spec = accuracy_score(nat_t, nat_p) if len(nat_t) > 0 else 1.0\n",
    "        return np.array([acc, f1, prec, rec, spec])\n",
    "\n",
    "    \n",
    "    actual_truths = [tr['truths'] for tr in test_results]\n",
    "    actual_preds = [tr['preds'] for tr in test_results]\n",
    "    actual_nats = [tr['is_nature'] for tr in test_results]\n",
    "    \n",
    "    \n",
    "    actual_scores = calc_all_metrics(actual_truths, actual_preds, actual_nats)\n",
    "    metric_names = [\"Accuracy\", \"Macro F1\", \"Macro Precision\", \"Macro Recall\", \"Nature Specificity\"]\n",
    "    \n",
    "    \n",
    "    print(f\"Running {n_boot} Bootstraps (Subject-Level)...\")\n",
    "    \n",
    "\n",
    "    subj_to_results = {s: [tr for tr in test_results if tr['subj'] == s] for s in unique_subjs}\n",
    "    boot_distributions = []\n",
    "    \n",
    "    for _ in tqdm(range(n_boot), desc=\"Bootstrapping\"):\n",
    "        sampled_subjs = np.random.choice(unique_subjs, size=len(unique_subjs), replace=True) \n",
    "        b_truths, b_preds, b_nats = [], [], []\n",
    "        \n",
    "        for s in sampled_subjs:\n",
    "            for tr in subj_to_results[s]:\n",
    "                b_truths.append(tr['truths'])\n",
    "                b_preds.append(tr['preds'])\n",
    "                b_nats.append(tr['is_nature'])\n",
    "                \n",
    "        boot_distributions.append(calc_all_metrics(b_truths, b_preds, b_nats))\n",
    "        \n",
    "    boot_distributions = np.array(boot_distributions)\n",
    "  \n",
    "    print(f\"Running {n_perm} Permutations (Sequence-Level)...\") \n",
    "    perm_distributions = []\n",
    "    \n",
    "    for _ in tqdm(range(n_perm), desc=\"Permuting\"):\n",
    "        \n",
    "        shuffled_truths = actual_truths.copy()\n",
    "        np.random.shuffle(shuffled_truths)\n",
    "        \n",
    "        perm_distributions.append(calc_all_metrics(shuffled_truths, actual_preds, actual_nats))\n",
    "        \n",
    "    perm_distributions = np.array(perm_distributions)\n",
    "    \n",
    "\n",
    "    print()\n",
    "    print(f\"{'Metric':<20} | {'Score':<6} | {'95% CI':<15} | {'p-value':<8} | {'Cohen d'}\")\n",
    "    print()\n",
    "    \n",
    "    for i, name in enumerate(metric_names):\n",
    "        actual = actual_scores[i]\n",
    "        \n",
    "       \n",
    "        ci_lower, ci_upper = np.percentile(boot_distributions[:, i], [2.5, 97.5])\n",
    "        \n",
    "        \n",
    "        perm_scores = perm_distributions[:, i]\n",
    "        p_val = np.sum(perm_scores >= actual) / n_perm\n",
    "        p_val_str = \"<0.0002\" if p_val == 0 else f\"{p_val:.4f}\"\n",
    "        \n",
    "        std_perm = np.std(perm_scores)\n",
    "    \n",
    "        cohens_d = (actual - np.mean(perm_scores)) / std_perm if std_perm > 0 else 0.0\n",
    "        \n",
    "        print(f\"{name:<20} | {actual:.4f} | [{ci_lower:.3f}, {ci_upper:.3f}] | {p_val_str:<8} | {cohens_d:.2f}\")\n",
    "    print()\n",
    "\n",
    "\n",
    "run_statistics(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96e1ad3b",
   "metadata": {},
   "source": [
    "### Multimodal Baseline: Temporal Probability Alignment\n",
    "\n",
    "**Methodology:**\n",
    "This section generates a qualitative visualization, termed a \"Tinglegram,\" to assess the temporal accuracy of the full multimodal fusion network's predictions against the subjective ground truth for a representative well-aligned participant (Subject 27). Panel A demonstrates the ASMR condition, while Panel B shows the Nature Control condition. This code assumes the main orchestrator is run, which saves the multimodal model's test outputs.\n",
    "\n",
    "\n",
    "This code outputs the Tinglegram figure for the manuscript. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "331f6c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import accuracy_score\n",
    "from scipy.ndimage import gaussian_filter1d\n",
    "import pickle\n",
    "def plot_smoothed_probability_timeline(test_results, target_subj=27):\n",
    "    print(f\"Extracting data for Subject {target_subj}...\")\n",
    "   \n",
    "    asmr_trial = next((item for item in test_results if item['subj'] == target_subj and item['is_nature'] == 0), None)\n",
    "    nature_trial = next((item for item in test_results if item['subj'] == target_subj and item['is_nature'] == 1), None)\n",
    "    \n",
    "    print(f\"  - ASMR Video   : {asmr_trial['vid_name']}\")\n",
    "    print(f\"  - Nature Video : {nature_trial['vid_name']}\")\n",
    "    \n",
    "    time_axis = np.arange(550) / 10.0  \n",
    "    \n",
    "    \n",
    "    asmr_user_scaled = asmr_trial['raw_truths']>0\n",
    "    nature_user_scaled = nature_trial['raw_truths'] / 3.0\n",
    "    \n",
    "    \n",
    "    asmr_ai_smooth = gaussian_filter1d(asmr_trial['probs'], sigma=2)\n",
    "    nature_ai_smooth = gaussian_filter1d(nature_trial['probs'], sigma=2)\n",
    "\n",
    "    \n",
    "    sns.set_theme(style=\"whitegrid\")\n",
    "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(10, 7), sharex=True)\n",
    "    plt.subplots_adjust(hspace=0.2)\n",
    "\n",
    "    \n",
    "    ax1.fill_between(time_axis, 0, asmr_user_scaled, color='lightgrey', step='post', label='User')\n",
    "    \n",
    "    ax1.plot(time_axis, asmr_ai_smooth, color='blue', linewidth=2.5, label='AI')\n",
    "    \n",
    "    ax1.axhline(0.5, color='red', linestyle='--', alpha=0.5, label='Threshold')\n",
    "    \n",
    "    ax1.set_title('ASMR', fontweight='bold')\n",
    "    ax1.set_ylabel('Intensity / Probability')\n",
    "    ax1.set_ylim(-0.05, 1.05)\n",
    "    ax1.legend(loc='upper right', frameon=True)\n",
    "\n",
    "   \n",
    "    ax2.fill_between(time_axis, 0, nature_user_scaled, color='lightgrey', step='post', label='User')\n",
    "\n",
    "    ax2.plot(time_axis, nature_ai_smooth, color='orange', linewidth=2.5, label='AI')\n",
    "  \n",
    "    ax2.axhline(0.5, color='red', linestyle='--', alpha=0.5, label='Threshold')\n",
    "    \n",
    "    ax2.set_title('Nature', fontweight='bold')\n",
    "    ax2.set_xlabel('Time (Seconds)')\n",
    "    ax2.set_ylabel('Intensity / Probability')\n",
    "    ax2.set_ylim(-0.05, 1.05)\n",
    "    ax2.legend(loc='upper right', frameon=True)\n",
    "\n",
    "    \n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "plot_smoothed_probability_timeline(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9925c8e8",
   "metadata": {},
   "source": [
    "### Modality Ablation Study\n",
    "\n",
    "**Methodology:**\n",
    "This section compares the full architecture against unimodal baselines (Video-only and Audio-only). \n",
    "\n",
    "To determine if the full fusion model provides a statistically significant improvement over the individual modalities, a **paired bootstrap significance test** (10,000 iterations) is conducted on the aggregated Macro F1-scores across the 4-fold cross-validation protocol. \n",
    "The empirical $p$-value is calculated based on the proportion of bootstrapped samples from the shifted null distribution that equal or exceed the actual observed performance gain. The hardcoded values were obtained at a representative run by the authors using random seed = 42.\n",
    "\n",
    "This code outputs the statistical significance metrics for manuscript table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "185932a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "def paired_bootstrap_test(model_a, model_b, n_boot=10000, seed=42):\n",
    "    \"\"\"\n",
    "    Performs a paired bootstrap test to check if model_a is significantly better than model_b.\n",
    "    \"\"\"\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "   \n",
    "    diffs = np.array(model_a) - np.array(model_b)\n",
    "    actual_mean_diff = np.mean(diffs)\n",
    "    \n",
    "   \n",
    "    shifted_diffs = diffs - actual_mean_diff\n",
    "    \n",
    "   \n",
    "    boot_mean_diffs = []\n",
    "    for _ in range(n_boot):\n",
    "        sample = np.random.choice(shifted_diffs, size=len(shifted_diffs), replace=True)\n",
    "        boot_mean_diffs.append(np.mean(sample))\n",
    "        \n",
    "    boot_mean_diffs = np.array(boot_mean_diffs)\n",
    "    \n",
    "    \n",
    "    p_val = np.mean(boot_mean_diffs >= actual_mean_diff)\n",
    "    \n",
    "    return actual_mean_diff, p_val\n",
    "\n",
    "\n",
    "multi = [0.7398, 0.7074, 0.8377, 0.5896]\n",
    "vid   = [0.7400, 0.7071, 0.7433, 0.5741]\n",
    "aud   = [0.7172, 0.7084, 0.7977, 0.5741]\n",
    "\n",
    "print(\"  STATISTICAL COMPARISON (MULTIMODAL vs ABLATIONS) \\n\")\n",
    "\n",
    "\n",
    "mean_diff_vid, p_boot_vid = paired_bootstrap_test(multi, vid)\n",
    "_, p_ttest_vid = stats.ttest_rel(multi, vid, alternative='greater')\n",
    "\n",
    "print(\"MULTIMODAL vs VIDEO-ONLY:\")\n",
    "print(f\"  Mean Improvement : +{mean_diff_vid:.4f}\")\n",
    "print(f\"  Bootstrap P-Value: {p_boot_vid:.4f}\")\n",
    "print(f\"  T-Test P-Value   : {p_ttest_vid:.4f}\\n\")\n",
    "\n",
    "\n",
    "mean_diff_aud, p_boot_aud = paired_bootstrap_test(multi, aud)\n",
    "_, p_ttest_aud = stats.ttest_rel(multi, aud, alternative='greater')\n",
    "\n",
    "print(\"MULTIMODAL vs AUDIO-ONLY:\")\n",
    "print(f\"  Mean Improvement : +{mean_diff_aud:.4f}\")\n",
    "print(f\"  Bootstrap P-Value: {p_boot_aud:.4f}\")\n",
    "print(f\"  T-Test P-Value   : {p_ttest_aud:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a73f32b1",
   "metadata": {},
   "source": [
    "### Video-Level Macro Classification & Statistical Inference\n",
    "\n",
    "**Methodology:**\n",
    "Since physiological triggers are often episodic, to evaluate the multimodal network's macro-level utility, this section applies a peak-detection strategy to the continuous predictions. An A Priori thresholding of top 15 percent frames determines the video's classification. Bootstrapping confirms the classifications' statistical validity.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "203687e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import pickle\n",
    "def run_video_level_classification(test_results, threshold_ratio=0.15):\n",
    "    print()\n",
    "    print(\"   VIDEO-LEVEL CLASSIFICATION (PEAK DETECTION STRATEGY)   \")\n",
    "    print()\n",
    "    \n",
    "    video_truths = []\n",
    "    video_preds = []\n",
    "    \n",
    "    frames_per_video = len(test_results[0]['preds']) \n",
    "    threshold_frames = int(frames_per_video * threshold_ratio) \n",
    "    \n",
    "    print(f\"Frames per video : {frames_per_video}\")\n",
    "    print(f\"Tingle Threshold : {threshold_frames} frames ({threshold_ratio*100}%)\\n\")\n",
    "    \n",
    "    for tr in test_results:\n",
    "        \n",
    "        vid_truth = 0 if tr['is_nature'] == 1 else 1\n",
    "        video_truths.append(vid_truth)\n",
    "        \n",
    "        \n",
    "        predicted_tingle_frames = np.sum(tr['preds'])\n",
    "        vid_pred = 1 if predicted_tingle_frames >= threshold_frames else 0\n",
    "        video_preds.append(vid_pred)\n",
    "        \n",
    "    \n",
    "    vid_acc = accuracy_score(video_truths, video_preds)\n",
    "    vid_prec, vid_rec, vid_f1, _ = precision_recall_fscore_support(\n",
    "        video_truths, video_preds, average='macro', zero_division=0)\n",
    "        \n",
    "    print(f\"Video-Level Accuracy        : {vid_acc:.4f} ({vid_acc*100:.2f}%)\")\n",
    "    print(f\"Video-Level Macro F1        : {vid_f1:.4f} ({vid_f1*100:.2f}%)\")\n",
    "    print(f\"Video-Level Macro Precision : {vid_prec:.4f}\")\n",
    "    print(f\"Video-Level Macro Recall    : {vid_rec:.4f}\")\n",
    "    \n",
    "    cm = confusion_matrix(video_truths, video_preds)\n",
    "    print(\"\\nConfusion Matrix (Video-Level):\")\n",
    "    print(f\"True Negatives (Nature correct) : {cm[0][0]}\")\n",
    "    print(f\"False Positives (Nature wrong)  : {cm[0][1]}\")\n",
    "    print(f\"False Negatives (ASMR missed)   : {cm[1][0]}\")\n",
    "    print(f\"True Positives (ASMR correct)   : {cm[1][1]}\")\n",
    "    print()\n",
    "def video_level_statistics(test_results, threshold_ratio=0.15, n_boot=2000, n_perm=5000):\n",
    "    print()\n",
    "    print(\" VIDEO-LEVEL STATISTICAL ANALYSIS (BOOTSTRAP & PERMUTATION) \")\n",
    "    print()\n",
    "    \n",
    "    frames_per_video = len(test_results[0]['preds'])\n",
    "    threshold_frames = int(frames_per_video * threshold_ratio)\n",
    "    \n",
    "    \n",
    "    vid_truths = []\n",
    "    vid_preds = []\n",
    "    \n",
    "    for tr in test_results:\n",
    "        truth = 0 if tr['is_nature'] == 1 else 1\n",
    "        pred = 1 if np.sum(tr['preds']) >= threshold_frames else 0\n",
    "        vid_truths.append(truth)\n",
    "        vid_preds.append(pred)\n",
    "        \n",
    "    vid_truths = np.array(vid_truths)\n",
    "    vid_preds = np.array(vid_preds)\n",
    "    \n",
    "    actual_acc = accuracy_score(vid_truths, vid_preds)\n",
    "    actual_f1 = f1_score(vid_truths, vid_preds, average='macro', zero_division=0)\n",
    "    \n",
    "   \n",
    "    print(f\"Running {n_boot} Video-Level Bootstraps...\")\n",
    "    np.random.seed(42)\n",
    "    boot_accs = []\n",
    "    \n",
    "    indices = np.arange(len(vid_truths))\n",
    "    for _ in range(n_boot):\n",
    "       \n",
    "        sample_idx = np.random.choice(indices, size=len(indices), replace=True)\n",
    "        sample_t = vid_truths[sample_idx]\n",
    "        sample_p = vid_preds[sample_idx]\n",
    "        boot_accs.append(accuracy_score(sample_t, sample_p))\n",
    "        \n",
    "    ci_lower, ci_upper = np.percentile(boot_accs, [2.5, 97.5])\n",
    "    \n",
    " \n",
    "    print(f\"Running {n_perm} Video-Level Permutations...\")\n",
    "    perm_accs = []\n",
    "    \n",
    "    for _ in range(n_perm):\n",
    "       \n",
    "        shuffled_t = vid_truths.copy()\n",
    "        np.random.shuffle(shuffled_t)\n",
    "        perm_accs.append(accuracy_score(shuffled_t, vid_preds))\n",
    "        \n",
    "    perm_accs = np.array(perm_accs)\n",
    "    p_val = np.sum(perm_accs >= actual_acc) / n_perm\n",
    "    p_val_str = \"< 0.0002\" if p_val == 0 else f\"{p_val:.4f}\"\n",
    "    \n",
    "    print(\"\\n--- FINAL VIDEO-LEVEL STATISTICS ---\")\n",
    "    print(f\"Actual Accuracy : {actual_acc*100:.2f}%\")\n",
    "    print(f\"Actual Macro F1 : {actual_f1*100:.2f}%\")\n",
    "    print(f\"95% CI (Acc)    : [{ci_lower*100:.2f}%, {ci_upper*100:.2f}%]\")\n",
    "    print(f\"P-Value         : {p_val_str}\")\n",
    "    print(\"-\" * 70)\n",
    "\n",
    "\n",
    "\n",
    "run_video_level_classification(results)\n",
    "\n",
    "\n",
    "\n",
    "video_level_statistics(results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
